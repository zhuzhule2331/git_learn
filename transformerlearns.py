import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Optional,Tuple

torch.manual_seed(42)
np.random.seed(42)#è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç æ¨¡å—:ä¸ºåºåˆ—ä¸­æ¯ä¸ªä½ç½®æ·»åŠ ä½ç½®ä¿¡æ¯
    ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ
    -transfomerä¸åƒRNNï¼Œæ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œæœ¬èº«æ— æ³•æ„ŸçŸ¥è¯çš„é¡ºåº
    -éœ€è¦æ˜¾ç¤ºçš„å‘Šè¯‰æ¨¡å‹æ¯ä¸ªè¯åœ¨åºåˆ—ä¸­çš„ä½ç½®
    
    ä½¿ç”¨åœºæ™¯ï¼š
    1.æ–‡æœ¬ï¼šç¼–ç è¯åœ¨å¥å­ä¸­çš„ä½ç½®
    2.å›¾åƒï¼šç¼–ç patchåœ¨å›¾åƒä¸­çš„ä½ç½®
    3.éŸ³é¢‘ï¼šç¼–ç å¸§åœ¨éŸ³é¢‘åºåˆ—ä¸­çš„ä½ç½®

    """
    def __init__(self,d_model:int,max_len:int=5000):
        """
        å‚æ•°è¯´æ˜ï¼š
        d_model:æ¨¡å‹çš„ç»´åº¦ï¼ˆä¾‹å¦‚512ï¼‰
        max_len:æ”¯æŒçš„åºåˆ—æœ€å¤§é•¿åº¦
        åœ¨ Transformer ä¸­ï¼Œè¾“å…¥çš„å¼ é‡å½¢çŠ¶é€šå¸¸æ˜¯ [batch_size, seq_len, d_model]ï¼Œå…¶ä¸­ï¼š
        batch_sizeï¼šæ‰¹æ¬¡å¤§å°
        seq_lenï¼šåºåˆ—é•¿åº¦ï¼ˆæ¯ä¸ªæ ·æœ¬çš„ token æ•°é‡ï¼‰
        d_modelï¼šæ¯ä¸ª token çš„ç‰¹å¾ç»´åº¦ï¼ˆæ¯”å¦‚ 512ï¼‰
        """        
        super(PositionalEncoding,self).__init__()
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ shape[max_len,d_model]
        pe =torch.zeros(max_len,d_model)
        # åˆ›å»ºä½ç½®ç´¢å¼•[0,1,2,...,max_len-1] shape[max_len,1]
        position = torch.arange(0,max_len).unsqueeze(1).float()

        # è®¡ç®—åˆ†æ¯é¡¹ï¼ˆç”¨äºè®¡ç®—æ­£å¼¦å’Œä½™å¼¦çš„å‘¨æœŸï¼‰
        # è¿™é‡Œä½¿ç”¨äº†ä¸€ä¸ªæŠ€å·§ï¼Œç”¨logå’Œexpé¿å…å¤§æ•°è¿ç®— shape[d_model/2]
        div_term = torch.exp(torch.arange(0,d_model,2).float()*
                             -(math.log(10000.0)/d_model))
        # å¶æ•°ç»´åº¦ä½¿ç”¨sin shape[max_len.d_model/2]
        pe[:,0::2] = torch.sin(position*div_term)
        # å¥‡æ•°ç»´åº¦ä½¿ç”¨cos shape[max_len,d_model/2]
        pe[:,1::2] = torch.cos(position*div_term)

        # å¢åŠ batchç»´åº¦ï¼Œå¹¶æ³¨å†Œä¸ºbufferï¼ˆä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼‰
        pe=pe.unsqueeze(0) #shape [1,max_len,d_model]
        self.register_buffer('pe',pe,False)
        #æŠŠä¸€ä¸ªå¼ é‡ï¼ˆæ¯”å¦‚è¿™é‡Œçš„ pe ä½ç½®ç¼–ç çŸ©é˜µï¼‰æ³¨å†Œä¸ºæ¨¡å‹çš„ç¼“å†²åŒºï¼ˆbufferï¼‰ 
        # â€”â€” å®ƒå±äºæ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼ˆä¼šéšæ¨¡å‹ç§»åŠ¨åˆ° GPU/CPUï¼‰ï¼Œä½†ä¸ä¼šè¢«ä¼˜åŒ–å™¨æ›´æ–°ï¼ˆéè®­ç»ƒå‚æ•°ï¼‰
        print(f"âœ”ï¸ä½ç½®ç¼–ç å®Œæˆ")
        print(f"æœ€å¤§åºåˆ—é•¿åº¦å¯¹åº”äºçš„seq_lenå•æ¡åºåˆ—ä¸­çš„æ ·æœ¬æ•°é‡{max_len}")
        print(f"æ¨¡å‹ç»´åº¦{d_model}")

    def forward(self,x:torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        è¾“å…¥ï¼š
        x[batch_size,seq_len,d_model]-è¾“å…¥çš„è¯çš„åµŒå…¥
        è¾“å‡ºï¼š
            [batch_size,seq_len,d_model]-æ·»åŠ ä½ç½®æ©ç åçš„ç»“æœ

        æ•°æ®æµç¤ºä¾‹ï¼š
        è¾“å…¥ x[32,100,512]#32ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª100ä¸ªè¯ï¼Œæ¯ä¸ªè¯512ç»´
        ä½ç½®ç¼–ç  [1,100,512]å‰100ä¸ªä½ç½®çš„ç¼–ç 
        ç›¸åŠ å¾—åˆ° [32ï¼Œ100ï¼Œ512] æ·»åŠ ä½ç½®ç¼–ç åçš„ç»“æœ

                    """
        seq_len = x.size(1)
        
        #è·å–å¯¹åº”é•¿åº¦çš„ä½ç½®ç¼–ç å¹¶ç›¸åŠ 
        #self.pe [:,:seq_len]çš„shape[1,seq_len,d_model]
        #å¹¿æ’­æœºåˆ¶ä¼šè‡ªåŠ¨æ‰©å±•åˆ°batchçº¬åº¦
        output = x +self.pe[:,:seq_len]

        return output

def scaled_dot_product_attention(
        query:torch.Tensor,
        key:torch.Tensor,
        value:torch.Tensor,
        mask:Optional[torch.Tensor]=None,
        drop_out:Optional[nn.Dropout] = None
) -> Tuple[torch.Tensor,torch.Tensor]:
    """
    ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶ï¼š
    å…¬å¼ï¼šAttention(Q,K,V) =softmax(Q*k^T/sqrt(d_k))V
        K^T:Kçš„è½¬èŒ
        d_k:Kçš„ç»´åº¦
    ä¸ºä¹ˆè¦è¿›è¡Œæ³¨æ„åŠ›ç¼©æ”¾ï¼Ÿ
        -å½“d_kå¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯ç»“æœä¼šå¾ˆå°
        -å¯¼è‡´softmaxåçš„æ¢¯åº¦å¾ˆå°ï¼Œè®­ç»ƒå›°éš¾
        -é™¤ä»¥sqrt(d_k)å¯ä»¥è§£å†³è¿™ç§é—®é¢˜
    å‚æ•°ï¼š
        query:[batch_size,n_heads,seq_len,d_k] -æŸ¥è¯¢çŸ©é˜µ
        key:[batch_size,n_heads,seq_len,d_k]-é”®çŸ©é˜µ
        value:[batch_size,n_heads,seq_len,d_v]- å€¼çŸ©é˜µ
        mask:[batch_size,1,1,seq_len] or [batch_size,1,seq_len,seq_len] -æ©ç 
        drop_out:Drop_outå±‚ï¼ˆå¯é€‰ï¼‰
            n_headsï¼šæ³¨æ„åŠ›å¤´çš„ä¸ªæ•°

    è¿”å›ï¼š
    output:[batch_size,n_heads,seq_len,d_v]-æ³¨æ„åŠ›è¾“å‡º
    attention_weights:[batch_size,n_heads,seq_len,seq_len]-æ³¨æ„åŠ›æƒé‡

    æ•°æ®æµç¤ºä¾‹ï¼š
        1.æœºå™¨ç¿»è¯‘ 
        åœºæ™¯ï¼šæˆ‘çˆ±åŒ—äº¬->I love Beijing.
        query:[32,8,10,64]#32ä¸ªæ ·æœ¬ï¼Œ8ä¸ªå¤´ï¼Œ10ä¸ªè¯ï¼ˆtokenï¼‰ï¼Œæ¯ä¸ªè¯64ç»´
        key:[32,8,10,64]
        value:[32,8,10,64]

        æ­¥éª¤1ï¼šQ*K^T ->[32,8,10,10]  # æ¯ä¸ªè¯å¯¹æ¯ä¸ªè¯çš„æ³¨æ„åŠ›åˆ†æ•°(k^Tæ˜¯Kçš„è½¬ç½®)
            è¡¡é‡æ¯ä¸ª â€œæŸ¥è¯¢ï¼ˆQï¼‰â€ å’Œæ‰€æœ‰ â€œé”®ï¼ˆKï¼‰â€ çš„åŒ¹é…ç¨‹åº¦ï¼ˆåˆ†æ•°è¶Šé«˜ï¼Œå…³è”è¶Šå¼ºï¼‰
            æ¯”å¦‚æ–‡æœ¬ä¸­ï¼šç¬¬ i ä¸ªè¯å¯¹ç¬¬ j ä¸ªè¯çš„æ³¨æ„åŠ›åˆ†æ•°
        æ­¥éª¤2ï¼šç¼©æ”¾ ->[32,8,10,10]/sqrt(64) = [32,8,10,10]/8
        æ­¥éª¤3ï¼šsoftmax -> [32,8,10,10] # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡ æŠŠæ³¨æ„åŠ›åˆ†æ•°è½¬åŒ–ä¸º â€œæƒé‡â€ï¼ˆæ¦‚ç‡ï¼‰ï¼Œè¡¨ç¤ºå¯¹æ¯ä¸ªä½ç½®çš„å…³æ³¨ç¨‹åº¦
        æ­¥éª¤4ï¼šä¹˜ä»¥V->[32,8,10,64] # åŠ æƒæ±‚å’Œå¾—åˆ°è¾“å‡º  [32,8,10,10] Ã— [32,8,10,64] â†’ [32,8,10,64]	
                ç”¨æ³¨æ„åŠ›æƒé‡å¯¹ â€œå€¼ï¼ˆVï¼‰â€ åŠ æƒï¼Œå¾—åˆ°èåˆäº†å…¨å±€å…³è”ä¿¡æ¯çš„è¾“å‡º
        
    """
    # è·å–æœ€åä¸€ä¸ªç»´åº¦çš„å¤§å°
    d_k = query.size(-1)

    #æ­¥éª¤1ï¼šè®¡ç®—Q*K^T
    # Q:[batch_size,n_headsï¼Œseq_len_q,d_k]
    # Kçš„è½¬ç½®:[batch_size,n_heads,d_k,seq_len_k]
    # score:[batc_size,n_heads,seq_len_q,seq_len_k]
    scores = torch.matmul(query,key.transpose(-2,-1))
    #æ­¥éª¤2ï¼šç¼©æ”¾
    scores = scores/math.sqrt(d_k)
    #æ­¥éª¤3ï¼šå¦‚æœæœ‰maskï¼Œåº”ç”¨maskï¼ˆåº”ç”¨äºmasked Attention)
    if mask is not None:
        #maskä¸º1çš„ä½ç½®è®¾ä¸º-inf,å¯¹åº”çš„softmaxä¸­çš„æ¦‚ç‡ä¼šå˜ä¸º0
        scores = scores.masked_fill(mask == 0,1e-9)
    #æ­¥éª¤4ï¼šSoftmaxå½’ä¸€åŒ–
    attention_weights = F.softmax(scores,dim=-1)
    #æ­¥éª¤5ï¼šå¦‚æœæœ‰dropout åº”ç”¨dropout
    if drop_out is not None:
        attention_weights =drop_out(attention_weights)
    #æ­¥éª¤6ï¼šä¹˜ä»¥Vå¾—åˆ°è¾“å‡º
    # attentinon_weights:[batch_size,,n_heads,seq_len_q,seq_len_k]
    # value:[batch_size,h_heads,seq_len_k,d_v]
    # output:[batch_size,heads,seq_len_q,d_v]
    output = torch.matmul(attention_weights,value)
    return output

def test_positional_encoding():
    """æµ‹è¯•ä½ç½®ç¼–ç æ¨¡å—"""
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯•ä½ç½®ç¼–ç ")
    print("="*50)
    
    batch_size = 2
    seq_len = 10
    d_model = 8
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"è¾“å…¥shape: {x.shape}")
    
    # åˆ›å»ºä½ç½®ç¼–ç å±‚
    pos_encoder = PositionalEncoding(d_model, max_len=100)
    
    # å‰å‘ä¼ æ’­
    output = pos_encoder(x)
    print(f"è¾“å‡ºshape: {output.shape}")
    print(f"âœ… ä½ç½®ç¼–ç æµ‹è¯•é€šè¿‡ï¼\n")
    
    return output

def test_attention():
    """æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶"""
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯•ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›")
    print("="*50)
    
    batch_size = 2
    n_heads = 4
    seq_len = 6
    d_k = 16
    
    # åˆ›å»ºQ, K, V
    Q = torch.randn(batch_size, n_heads, seq_len, d_k)
    K = torch.randn(batch_size, n_heads, seq_len, d_k)
    V = torch.randn(batch_size, n_heads, seq_len, d_k)
    
    print(f"Q shape: {Q.shape}")
    print(f"K shape: {K.shape}")
    print(f"V shape: {V.shape}")
    
    # è®¡ç®—æ³¨æ„åŠ›
    output, weights = scaled_dot_product_attention(Q, K, V)
    
    print(f"è¾“å‡º shape: {output.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡ shape: {weights.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡å’Œ: {weights[0, 0, 0].sum():.4f} (åº”è¯¥æ¥è¿‘1.0)")
    print(f"âœ… æ³¨æ„åŠ›æœºåˆ¶æµ‹è¯•é€šè¿‡ï¼\n")
    
    return output, weights




def check_cuda_torch_info():
    torch_version = torch.__version__
    print(f"(â—'â—¡'â—)ğŸ”torchçš„ç‰ˆæœ¬:{torch_version}")
    cuda_venv_version = torch.version.cuda
    print(f"ğŸ”è™šæ‹Ÿç¯å¢ƒçš„cudaç‰ˆæœ¬:{cuda_venv_version}")
    cuda_aviable = torch.cuda.is_available()
    print(f"cuda å¯ç”¨ğŸ‘Œ" if cuda_aviable else "cuda ä¸å¯ç”¨ğŸ˜’")
    if cuda_aviable:
        gpu_count =torch.cuda.device_count()
        print(f"å¯ç”¨æ ¸å¿ƒæ•°ï¼š{gpu_count}")
        current_device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_device)
        print(f"å½“å‰Gpuç´¢å¼•{current_device},åç§°{gpu_name}")
        cuda_runtime_version = torch.backends.cudnn.version()
        print(f"cudnnçš„ç‰ˆæœ¬æ˜¯{cuda_runtime_version}")



if __name__ == '__main__':
    _ = test_positional_encoding()
    # å…ˆæ¿€æ´»ç¯å¢ƒï¼Œå†è¿è¡Œpython

    # è¿è¡Œæµ‹è¯•
    _ = test_attention()

