import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Optional,Tuple

torch.manual_seed(42)
np.random.seed(42)#è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç æ¨¡å—:ä¸ºåºåˆ—ä¸­æ¯ä¸ªä½ç½®æ·»åŠ ä½ç½®ä¿¡æ¯
    ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ
    -transfomerä¸åƒRNNï¼Œæ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œæœ¬èº«æ— æ³•æ„ŸçŸ¥è¯çš„é¡ºåº
    -éœ€è¦æ˜¾ç¤ºçš„å‘Šè¯‰æ¨¡å‹æ¯ä¸ªè¯åœ¨åºåˆ—ä¸­çš„ä½ç½®
    
    ä½¿ç”¨åœºæ™¯ï¼š
    1.æ–‡æœ¬ï¼šç¼–ç è¯åœ¨å¥å­ä¸­çš„ä½ç½®
    2.å›¾åƒï¼šç¼–ç patchåœ¨å›¾åƒä¸­çš„ä½ç½®
    3.éŸ³é¢‘ï¼šç¼–ç å¸§åœ¨éŸ³é¢‘åºåˆ—ä¸­çš„ä½ç½®

    """
    def __init__(self,d_model:int,max_len:int=5000):
        """
        å‚æ•°è¯´æ˜ï¼š
        d_model:æ¨¡å‹çš„ç»´åº¦ï¼ˆä¾‹å¦‚512ï¼‰
        max_len:æ”¯æŒçš„åºåˆ—æœ€å¤§é•¿åº¦
        åœ¨ Transformer ä¸­ï¼Œè¾“å…¥çš„å¼ é‡å½¢çŠ¶é€šå¸¸æ˜¯ [batch_size, seq_len, d_model]ï¼Œå…¶ä¸­ï¼š
        batch_sizeï¼šæ‰¹æ¬¡å¤§å°
        seq_lenï¼šåºåˆ—é•¿åº¦ï¼ˆæ¯ä¸ªæ ·æœ¬çš„ token æ•°é‡ï¼‰
        d_modelï¼šæ¯ä¸ª token çš„ç‰¹å¾ç»´åº¦ï¼ˆæ¯”å¦‚ 512ï¼‰
        """        
        super(PositionalEncoding,self).__init__()
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ shape[max_len,d_model]
        pe =torch.zeros(max_len,d_model)
        # åˆ›å»ºä½ç½®ç´¢å¼•[0,1,2,...,max_len-1] shape[max_len,1]
        position = torch.arange(0,max_len).unsqueeze(1).float()

        # è®¡ç®—åˆ†æ¯é¡¹ï¼ˆç”¨äºè®¡ç®—æ­£å¼¦å’Œä½™å¼¦çš„å‘¨æœŸï¼‰
        # è¿™é‡Œä½¿ç”¨äº†ä¸€ä¸ªæŠ€å·§ï¼Œç”¨logå’Œexpé¿å…å¤§æ•°è¿ç®— shape[d_model/2]
        div_term = torch.exp(torch.arange(0,d_model,2).float()*
                             -(math.log(10000.0)/d_model))
        # å¶æ•°ç»´åº¦ä½¿ç”¨sin shape[max_len.d_model/2]
        pe[:,0::2] = torch.sin(position*div_term)
        # å¥‡æ•°ç»´åº¦ä½¿ç”¨cos shape[max_len,d_model/2]
        pe[:,1::2] = torch.cos(position*div_term)

        # å¢åŠ batchç»´åº¦ï¼Œå¹¶æ³¨å†Œä¸ºbufferï¼ˆä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼‰
        pe=pe.unsqueeze(0) #shape [1,max_len,d_model]
        self.register_buffer('pe',pe,False)
        #æŠŠä¸€ä¸ªå¼ é‡ï¼ˆæ¯”å¦‚è¿™é‡Œçš„ pe ä½ç½®ç¼–ç çŸ©é˜µï¼‰æ³¨å†Œä¸ºæ¨¡å‹çš„ç¼“å†²åŒºï¼ˆbufferï¼‰ 
        # â€”â€” å®ƒå±äºæ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼ˆä¼šéšæ¨¡å‹ç§»åŠ¨åˆ° GPU/CPUï¼‰ï¼Œä½†ä¸ä¼šè¢«ä¼˜åŒ–å™¨æ›´æ–°ï¼ˆéè®­ç»ƒå‚æ•°ï¼‰
        print(f"âœ”ï¸ä½ç½®ç¼–ç å®Œæˆ")
        print(f"æœ€å¤§åºåˆ—é•¿åº¦å¯¹åº”äºçš„seq_lenå•æ¡åºåˆ—ä¸­çš„æ ·æœ¬æ•°é‡{max_len}")
        print(f"æ¨¡å‹ç»´åº¦{d_model}")

    def forward(self,x:torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        è¾“å…¥ï¼š
        x[batch_size,seq_len,d_model]-è¾“å…¥çš„è¯çš„åµŒå…¥
        è¾“å‡ºï¼š
            [batch_size,seq_len,d_model]-æ·»åŠ ä½ç½®æ©ç åçš„ç»“æœ

        æ•°æ®æµç¤ºä¾‹ï¼š
        è¾“å…¥ x[32,100,512]#32ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª100ä¸ªè¯ï¼Œæ¯ä¸ªè¯512ç»´
        ä½ç½®ç¼–ç  [1,100,512]å‰100ä¸ªä½ç½®çš„ç¼–ç 
        ç›¸åŠ å¾—åˆ° [32ï¼Œ100ï¼Œ512] æ·»åŠ ä½ç½®ç¼–ç åçš„ç»“æœ

                    """
        seq_len = x.size(1)
        
        #è·å–å¯¹åº”é•¿åº¦çš„ä½ç½®ç¼–ç å¹¶ç›¸åŠ 
        #self.pe [:,:seq_len]ä¼šæ‰©å±•åˆ°shape[1,seq_len,d_model]
        #å¹¿æ’­æœºåˆ¶ä¼šè‡ªåŠ¨æ‰©å±•åˆ°batchçº¬åº¦
        output = x +self.pe[:,:seq_len]

        return output

def scaled_dot_product_attention(
        query:torch.Tensor,
        key:torch.Tensor,
        value:torch.Tensor,
        mask:Optional[torch.Tensor]=None,
        drop_out:Optional[nn.Dropout] = None
) -> Tuple[torch.Tensor,torch.Tensor]:
    """
    ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶ï¼š
    å…¬å¼ï¼šAttention(Q,K,V) =softmax(Q*k^T/sqrt(d_k))V
        K^T:Kçš„è½¬èŒ
        d_k:Kçš„ç»´åº¦
    ä¸ºä¹ˆè¦è¿›è¡Œæ³¨æ„åŠ›ç¼©æ”¾ï¼Ÿ
        -å½“d_kå¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯ç»“æœä¼šå¾ˆå°
        -å¯¼è‡´softmaxåçš„æ¢¯åº¦å¾ˆå°ï¼Œè®­ç»ƒå›°éš¾
        -é™¤ä»¥sqrt(d_k)å¯ä»¥è§£å†³è¿™ç§é—®é¢˜
    å‚æ•°ï¼š
        query:[batch_size,n_heads,seq_len,d_k] -æŸ¥è¯¢çŸ©é˜µ
        key:[batch_size,n_heads,seq_len,d_k]-é”®çŸ©é˜µ
        value:[batch_size,n_heads,seq_len,d_v]- å€¼çŸ©é˜µ
        mask:[batch_size,1,1,seq_len] or [batch_size,1,seq_len,seq_len] -æ©ç 
        drop_out:Drop_outå±‚ï¼ˆå¯é€‰ï¼‰
            n_headsï¼šæ³¨æ„åŠ›å¤´çš„ä¸ªæ•°

    è¿”å›ï¼š
    output:[batch_size,n_heads,seq_len,d_v]-æ³¨æ„åŠ›è¾“å‡º
    attention_weights:[batch_size,n_heads,seq_len,seq_len]-æ³¨æ„åŠ›æƒé‡

    æ•°æ®æµç¤ºä¾‹ï¼š
        1.æœºå™¨ç¿»è¯‘ 
        åœºæ™¯ï¼šæˆ‘çˆ±åŒ—äº¬->I love Beijing.
        query:[32,8,10,64]#32ä¸ªæ ·æœ¬ï¼Œ8ä¸ªå¤´ï¼Œ10ä¸ªè¯ï¼ˆtokenï¼‰ï¼Œæ¯ä¸ªè¯64ç»´
        key:[32,8,10,64]
        value:[32,8,10,64]

        æ­¥éª¤1ï¼šQ*K^T ->[32,8,10,10]  # æ¯ä¸ªè¯å¯¹æ¯ä¸ªè¯çš„æ³¨æ„åŠ›åˆ†æ•°(k^Tæ˜¯Kçš„è½¬ç½®)
            è¡¡é‡æ¯ä¸ª â€œæŸ¥è¯¢ï¼ˆQï¼‰â€ å’Œæ‰€æœ‰ â€œé”®ï¼ˆKï¼‰â€ çš„åŒ¹é…ç¨‹åº¦ï¼ˆåˆ†æ•°è¶Šé«˜ï¼Œå…³è”è¶Šå¼ºï¼‰
            æ¯”å¦‚æ–‡æœ¬ä¸­ï¼šç¬¬ i ä¸ªè¯å¯¹ç¬¬ j ä¸ªè¯çš„æ³¨æ„åŠ›åˆ†æ•°
        æ­¥éª¤2ï¼šç¼©æ”¾ ->[32,8,10,10]/sqrt(64) = [32,8,10,10]/8
        æ­¥éª¤3ï¼šsoftmax -> [32,8,10,10] # å½’ä¸€åŒ–ä¸ºæ¦‚ç‡ æŠŠæ³¨æ„åŠ›åˆ†æ•°è½¬åŒ–ä¸º â€œæƒé‡â€ï¼ˆæ¦‚ç‡ï¼‰ï¼Œè¡¨ç¤ºå¯¹æ¯ä¸ªä½ç½®çš„å…³æ³¨ç¨‹åº¦
        æ­¥éª¤4ï¼šä¹˜ä»¥V->[32,8,10,64] # åŠ æƒæ±‚å’Œå¾—åˆ°è¾“å‡º  [32,8,10,10] Ã— [32,8,10,64] â†’ [32,8,10,64]	
                ç”¨æ³¨æ„åŠ›æƒé‡å¯¹ â€œå€¼ï¼ˆVï¼‰â€ åŠ æƒï¼Œå¾—åˆ°èåˆäº†å…¨å±€å…³è”ä¿¡æ¯çš„è¾“å‡º
        
    """
    # è·å–æœ€åä¸€ä¸ªç»´åº¦çš„å¤§å°
    d_k = query.size(-1)

    #æ­¥éª¤1ï¼šè®¡ç®—Q*K^T
    # Q:[batch_size,n_headsï¼Œseq_len_q,d_k]
    # Kçš„è½¬ç½®:[batch_size,n_heads,d_k,seq_len_k]
    # score:[batc_size,n_heads,seq_len_q,seq_len_k]
    scores = torch.matmul(query,key.transpose(-2,-1))
    #æ­¥éª¤2ï¼šç¼©æ”¾
    scores = scores/math.sqrt(d_k)
    #æ­¥éª¤3ï¼šå¦‚æœæœ‰maskï¼Œåº”ç”¨maskï¼ˆåº”ç”¨äºmasked Attention)
    if mask is not None:
        #maskä¸º1çš„ä½ç½®è®¾ä¸º-inf,å¯¹åº”çš„softmaxä¸­çš„æ¦‚ç‡ä¼šå˜ä¸º0
        scores = scores.masked_fill(mask == 0,-1e9)
    #æ­¥éª¤4ï¼šSoftmaxå½’ä¸€åŒ–
    attention_weights = F.softmax(scores,dim=-1)
    #æ­¥éª¤5ï¼šå¦‚æœæœ‰dropout åº”ç”¨dropout
    if drop_out is not None:
        attention_weights =drop_out(attention_weights)
    #æ­¥éª¤6ï¼šä¹˜ä»¥Vå¾—åˆ°è¾“å‡º
    # attentinon_weights:[batch_size,,n_heads,seq_len_q,seq_len_k]
    # value:[batch_size,h_heads,seq_len_k,d_v]
    # output:[batch_size,heads,seq_len_q,d_v]
    output = torch.matmul(attention_weights,value)
    return output,attention_weights

def test_positional_encoding():
    """æµ‹è¯•ä½ç½®ç¼–ç æ¨¡å—"""
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯•ä½ç½®ç¼–ç ")
    print("="*50)
    
    batch_size = 2
    seq_len = 10
    d_model = 8
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"è¾“å…¥shape: {x.shape}")
    
    # åˆ›å»ºä½ç½®ç¼–ç å±‚
    pos_encoder = PositionalEncoding(d_model, max_len=100)
    
    # å‰å‘ä¼ æ’­
    output = pos_encoder(x)
    print(f"è¾“å‡ºshape: {output.shape}")
    print(f"âœ… ä½ç½®ç¼–ç æµ‹è¯•é€šè¿‡ï¼\n")
    
    return output

def test_attention():
    """æµ‹è¯•æ³¨æ„åŠ›æœºåˆ¶"""
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯•ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›")
    print("="*50)
    
    batch_size = 2
    n_heads = 4
    seq_len = 6
    d_k = 16
    
    # åˆ›å»ºQ, K, V
    Q = torch.randn(batch_size, n_heads, seq_len, d_k)
    K = torch.randn(batch_size, n_heads, seq_len, d_k)
    V = torch.randn(batch_size, n_heads, seq_len, d_k)
    
    print(f"Q shape: {Q.shape}")
    print(f"K shape: {K.shape}")
    print(f"V shape: {V.shape}")
    
    # è®¡ç®—æ³¨æ„åŠ›
    output, weights = scaled_dot_product_attention(Q, K, V)
    
    print(f"è¾“å‡º shape: {output.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡ shape: {weights.shape}")
    print(weights)
    print(f"æ³¨æ„åŠ›æƒé‡å’Œ: {weights[0, 0, 0].sum():.4f} (åº”è¯¥æ¥è¿‘1.0)")
    print(f"âœ… æ³¨æ„åŠ›æœºåˆ¶æµ‹è¯•é€šè¿‡ï¼\n")
    
    return output, weights

def check_cuda_torch_info():
    torch_version = torch.__version__
    print(f"(â—'â—¡'â—)ğŸ”torchçš„ç‰ˆæœ¬:{torch_version}")
    cuda_venv_version = torch.version.cuda
    print(f"ğŸ”è™šæ‹Ÿç¯å¢ƒçš„cudaç‰ˆæœ¬:{cuda_venv_version}")
    cuda_aviable = torch.cuda.is_available()
    print(f"cuda å¯ç”¨ğŸ‘Œ" if cuda_aviable else "cuda ä¸å¯ç”¨ğŸ˜’")
    if cuda_aviable:
        gpu_count =torch.cuda.device_count()
        print(f"å¯ç”¨æ ¸å¿ƒæ•°ï¼š{gpu_count}")
        current_device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_device)
        print(f"å½“å‰Gpuç´¢å¼•{current_device},åç§°{gpu_name}")
        cuda_runtime_version = torch.backends.cudnn.version()
        print(f"cudnnçš„ç‰ˆæœ¬æ˜¯{cuda_runtime_version}")

class MultiHeadAttention(nn.Module):
    """
    å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
    æ ¸å¿ƒæ€æƒ³ï¼š
    -å•ä¸ªæ³¨æ„åŠ›å¯èƒ½åªå…³æ³¨æŸä¸€ç§å…³ç³»
    -å¤šä¸ªæ³¨æ„åŠ›å¯èƒ½å…³æ³¨ä¸åŒçš„å…³ç³»
    -æœ€åå°†æ‰€æœ‰çš„å¤´çš„è¾“å‡ºæ‹¼æ¥èµ·æ¥
    ä½¿ç”¨åœºæ™¯
    -æ–‡æœ¬ï¼šä¸åŒå¤´å…³æ³¨è¯­æ³•ï¼Œè¯­ä¹‰ï¼Œé•¿è·ç¦»ä¾èµ–ç­‰
    -å›¾åƒï¼šä¸åŒå¤´å…³æ³¨çº¹ç†ï¼Œé¢œè‰²ï¼Œå½¢çŠ¶ç­‰
    -å¤šæ¨¡æ€ï¼šä¸åŒå¤´å…³æ³¨æ¨¡æ€å†…å’Œæ¨¡æ€é—´çš„å…³ç³»
    """
    def __init__(self,d_model:int,n_heads:int,dropout:float = 0.1):
        """
        å‚æ•°
        d_model:æ¨¡å‹ç»´åº¦ï¼ˆå¿…é¡»èƒ½è¢«n_headsæ•´é™¤ï¼‰
        n_head:æ³¨æ„åŠ›å¤´æ•°
        dropout:Dropoutæ¦‚ç‡"""
        super(MultiHeadAttention,self).__init__()

        assert d_model%n_heads==0,f"d_model({d_model})å¿…é¡»è¦è¢«n_heads({n_heads})æ•´é™¤"
        self.d_model=d_model
        self.n_heads=n_heads
        self.d_k=d_model//n_heads

        #åˆ›å»ºQï¼ŒKï¼ŒVçš„çº¿æ€§å˜æ¢å±‚ï¼ˆè¿™é‡Œä½¿ç”¨nn.Linear,ä¸ç”¨é«˜çº§APIï¼‰
        #ä¸ºä»€ä¹ˆæ˜¯4ä¸ªnn.Linear?
        #ä¸‰ä¸ªç”¨äºç”ŸæˆQï¼ŒKï¼ŒV
        #å‰©ä¸‹ä¸€ä¸ªç”¨äºæœ€åçš„æ˜ å°„è¾“å‡º
        self.w_q = nn.Linear(d_model,d_model,bias=False) #shape[d_model,d_model]
        self.w_k = nn.Linear(d_model,d_model,bias=False) #shape[d_model,d_model]
        self.w_v = nn.Linear(d_model,d_model,bias=False) #shape[d_model,d_model]
        self.w_o = nn.Linear(d_model,d_model,bias=False) #shape[d_model,d_model]

        self.dropout =nn.Dropout(dropout)
        #åˆå§‹åŒ–æƒé‡
        self._init_weights()

        print("å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶åˆå§‹åŒ–å®Œæˆ")
        print(f"    æ¨¡å‹ç»´åº¦{d_model}")
        print(f"    æ³¨æ„åŠ›å¤´æ•°{n_heads}")
        print(f"    æ¯ä¸ªå¤´çš„ç»´åº¦{self.d_k}")

    def _init_weights(self):
            """Xavieråˆå§‹åŒ–æƒé‡"""
            for module in [self.w_q,self.w_k,self.w_v,self.w_o]:
                nn.init.xavier_uniform_(module.weight)
    
    def forward(self,
                query:torch.Tensor,
                key:torch.Tensor,
                value:torch.Tensor,
                mask:Optional[torch.tensor]=None
                )->Tuple[torch.Tensor,torch.Tensor]:
        """å‰å‘ä¼ æ’­
        è¾“å…¥ï¼š
            query:[batch_size,seq_len_q,d_model]
            key:[batch_size,seq_len_k,d_model]
            value:[batch_size,seq_len_v,d_model]
            mask:[batch_size,1,1,seq_len] or None
            
        è¾“å‡º:
            output:[batch_size,seq_len_q,d_model]
            attention_weights[batch_size,seq_len_q,seq_len_k]
        
        æ•°æ®æµç¤ºä¾‹ï¼š
        æ–‡æœ¬ç¿»è¯‘â€œHello,World!(2ä¸ªè¯)"
        Qï¼ŒKï¼ŒV[32,2,512] 32æ ·æœ¬ï¼Œ2è¯ï¼Œ512ç»´

        æ­¥éª¤1ï¼šçº¿æ€§å˜æ¢ï¼ˆä¼ªä»£ç ï¼‰
            Q = w_q*query -> [32,2,512]
            K = w_k*key -> [32,2,512]
            V = w_v*value ->[32,2,512]

        æ­¥éª¤2ï¼šåˆ†å¤´ï¼ˆreshape+transpose
            Q->[32,2,512]->[32,2,8,64]->[32,8,2,64] #å…«ä¸ªå¤´ï¼Œæ¯ä¸ªå¤´64ç»´
            K->[32,2,512]->[32,2,8,64]->[32,8,2,64]
            V->[32,2,512]->[32,2,8,64]->[32,8,2,64]
        æ­¥éª¤3ï¼šè®¡ç®—æ³¨æ„åŠ›
            æ¯ä¸ªå¤´ç‹¬ç«‹è®¡ç®—æ³¨æ„åŠ›  -> [32,8,2,64]
        æ­¥éª¤4ï¼šæ‹¼æ¥æ‰€æœ‰å¤´
            [32,8,2,64]->[32,2,8,64]->[32,2,512]
        æ­¥éª¤5ï¼šæœ€ç»ˆçº¿æ€§å˜æ¢ï¼ˆä¼ªä»£ç ï¼Œå®é™…ä½¿ç”¨self.w_o(output)Linearç±»ä¸­æ–¹æ³•ï¼‰
            w_o * concat ->[32,2,512]
            
            """
        batch_size=query.size(0)
        seq_len_q =query.size(1)
        #æ­¥éª¤1çº¿æ€§å˜æ¢ç”ŸæˆQï¼ŒKï¼ŒV
        #[batch_size,seq_len,d_model]->[batch_size,seq_len,d_model]
        Q = self.w_q(query) # å‡è®¾ä¸º[32,10,512]
        K = self.w_k(key)
        V = self.w_v(value)

        #æ­¥éª¤2ï¼šåˆ†å¤´
        #[batch_size,seq_len,d_model]->[batch_size,seq_len,n_heads,d_k]->[batch_size,n_heads,seq_len,d_k]
        Q = Q.view(batch_size,-1,self.n_heads,self.d_k).transpose(1,2) 
        K = K.view(batch_size,-1,self.n_heads,self.d_k).transpose(1,2)
        V = V.view(batch_size,-1,self.n_heads,self.d_k).transpose(1,2)
        # ç°åœ¨shapeå˜ä¸º[32,8,10,64]ï¼Œ-1æ˜¯è‡ªåŠ¨è®¡ç®—ä½æ•°ã€‚

        #æ­¥éª¤3ï¼šè®¡ç®—ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
        #[batch_size,n_heads,seq_len_q,d_k]->[batch_size,n_heads,seq_len,d_k]
        attention_output,attention_weights=scaled_dot_product_attention(Q,K,V,mask,self.dropout)
        #output:[32,8,10,64]
        #attention_weights[32,8,10,10]

        #æ­¥éª¤4ï¼š æ‹¼æ¥å¤šå¤´è¾“å‡º
        #[batch_size,n_heads,seq_len_q,d_k]->[batch_size,seq_len_q,n_heads,d_k]
        # ->[batch_size,seq_len,d_model]
        attention_output=attention_output.transpose(1,2).contiguous().view(batch_size,seq_len_q,self.d_model)
        #ç°åœ¨[32,10,512]

        #æ­¥éª¤5ï¼šæœ€ç»ˆçš„çº¿æ€§å˜æ¢
        output = self.w_o(attention_output)
        #output[32,10,512]
        return output,attention_weights
    
# æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›
def test_multihead_attention():
    """æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶"""
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›")
    print("="*50)
    
    batch_size = 2
    seq_len = 5
    d_model = 256
    n_heads = 8
    
    # åˆ›å»ºè¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"è¾“å…¥ shape: {x.shape}")
    
    # åˆ›å»ºå¤šå¤´æ³¨æ„åŠ›å±‚
    mha = MultiHeadAttention(d_model, n_heads)
    
    # è‡ªæ³¨æ„åŠ›ï¼šQ=K=V
    output, weights = mha(x, x, x)
    
    print(f"è¾“å‡º shape: {output.shape}")
    print(f"æ³¨æ„åŠ›æƒé‡ shape: {weights.shape}")
    print(f"âœ… å¤šå¤´æ³¨æ„åŠ›æµ‹è¯•é€šè¿‡ï¼\n")
    
    return output, weights



if __name__ == '__main__':
    _ = test_positional_encoding()
    # å…ˆæ¿€æ´»ç¯å¢ƒï¼Œå†è¿è¡Œpython

    # è¿è¡Œæµ‹è¯•
    _ = test_attention()
    # è¿è¡Œæµ‹è¯•
    _ = test_multihead_attention() 
