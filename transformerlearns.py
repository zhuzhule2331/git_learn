import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Optional,Tuple

torch.manual_seed(42)
np.random.seed(42)#è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°

class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç æ¨¡å—:ä¸ºåºåˆ—ä¸­æ¯ä¸ªä½ç½®æ·»åŠ ä½ç½®ä¿¡æ¯
    ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ
    -transfomerä¸åƒRNNï¼Œæ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œæœ¬èº«æ— æ³•æ„ŸçŸ¥è¯çš„é¡ºåº
    -éœ€è¦æ˜¾ç¤ºçš„å‘Šè¯‰æ¨¡å‹æ¯ä¸ªè¯åœ¨åºåˆ—ä¸­çš„ä½ç½®
    
    ä½¿ç”¨åœºæ™¯ï¼š
    1.æ–‡æœ¬ï¼šç¼–ç è¯åœ¨å¥å­ä¸­çš„ä½ç½®
    2.å›¾åƒï¼šç¼–ç patchåœ¨å›¾åƒä¸­çš„ä½ç½®
    3.éŸ³é¢‘ï¼šç¼–ç å¸§åœ¨éŸ³é¢‘åºåˆ—ä¸­çš„ä½ç½®

    """
    def __init__(self,d_model:int,max_len:int=5000):
        """
        å‚æ•°è¯´æ˜ï¼š
        d_model:æ¨¡å‹çš„ç»´åº¦ï¼ˆä¾‹å¦‚512ï¼‰
        max_len:æ”¯æŒçš„åºåˆ—æœ€å¤§é•¿åº¦
        åœ¨ Transformer ä¸­ï¼Œè¾“å…¥çš„å¼ é‡å½¢çŠ¶é€šå¸¸æ˜¯ [batch_size, seq_len, d_model]ï¼Œå…¶ä¸­ï¼š
        batch_sizeï¼šæ‰¹æ¬¡å¤§å°
        seq_lenï¼šåºåˆ—é•¿åº¦ï¼ˆæ¯ä¸ªæ ·æœ¬çš„ token æ•°é‡ï¼‰
        d_modelï¼šæ¯ä¸ª token çš„ç‰¹å¾ç»´åº¦ï¼ˆæ¯”å¦‚ 512ï¼‰
        """        
        super(PositionalEncoding,self).__init__()
        # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ shape[max_len,d_model]
        pe =torch.zeros(max_len,d_model)
        # åˆ›å»ºä½ç½®ç´¢å¼•[0,1,2,...,max_len-1] shape[max_len,1]
        position = torch.arange(0,max_len).unsqueeze(1).float()

        # è®¡ç®—åˆ†æ¯é¡¹ï¼ˆç”¨äºè®¡ç®—æ­£å¼¦å’Œä½™å¼¦çš„å‘¨æœŸï¼‰
        # è¿™é‡Œä½¿ç”¨äº†ä¸€ä¸ªæŠ€å·§ï¼Œç”¨logå’Œexpé¿å…å¤§æ•°è¿ç®— shape[d_model/2]
        div_term = torch.exp(torch.arange(0,d_model,2).float()*
                             -(math.log(10000.0)/d_model))
        # å¶æ•°ç»´åº¦ä½¿ç”¨sin shape[max_len.d_model/2]
        pe[:,0::2] = torch.sin(position*div_term)
        # å¥‡æ•°ç»´åº¦ä½¿ç”¨cos shape[max_len,d_model/2]
        pe[:,1::2] = torch.cos(position*div_term)

        # å¢åŠ batchç»´åº¦ï¼Œå¹¶æ³¨å†Œä¸ºbufferï¼ˆä¸å‚ä¸æ¢¯åº¦æ›´æ–°ï¼‰
        pe=pe.unsqueeze(0) #shape [1,max_len,d_model]
        self.register_buffer('pe',pe,False)
        #æŠŠä¸€ä¸ªå¼ é‡ï¼ˆæ¯”å¦‚è¿™é‡Œçš„ pe ä½ç½®ç¼–ç çŸ©é˜µï¼‰æ³¨å†Œä¸ºæ¨¡å‹çš„ç¼“å†²åŒºï¼ˆbufferï¼‰ 
        # â€”â€” å®ƒå±äºæ¨¡å‹çš„ä¸€éƒ¨åˆ†ï¼ˆä¼šéšæ¨¡å‹ç§»åŠ¨åˆ° GPU/CPUï¼‰ï¼Œä½†ä¸ä¼šè¢«ä¼˜åŒ–å™¨æ›´æ–°ï¼ˆéè®­ç»ƒå‚æ•°ï¼‰
        print(f"âœ”ï¸ä½ç½®ç¼–ç å®Œæˆ")
        print(f"æœ€å¤§åºåˆ—é•¿åº¦å¯¹åº”äºçš„seq_lenå•æ¡åºåˆ—ä¸­çš„æ ·æœ¬æ•°é‡{max_len}")
        print(f"æ¨¡å‹ç»´åº¦{d_model}")

    def forward(self,x:torch.Tensor) -> torch.Tensor:
        """
        å‰å‘ä¼ æ’­
        è¾“å…¥ï¼š
        x[batch_size,seq_len,d_model]-è¾“å…¥çš„è¯çš„åµŒå…¥
        è¾“å‡ºï¼š
            [batch_size,seq_len,d_model]-æ·»åŠ ä½ç½®æ©ç åçš„ç»“æœ

        æ•°æ®æµç¤ºä¾‹ï¼š
        è¾“å…¥ x[32,100,512]#32ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª100ä¸ªè¯ï¼Œæ¯ä¸ªè¯512ç»´
        ä½ç½®ç¼–ç  [1,100,512]å‰100ä¸ªä½ç½®çš„ç¼–ç 
        ç›¸åŠ å¾—åˆ° [32ï¼Œ100ï¼Œ512] æ·»åŠ ä½ç½®ç¼–ç åçš„ç»“æœ

                    """
        seq_len = x.size(1)
        
        #è·å–å¯¹åº”é•¿åº¦çš„ä½ç½®ç¼–ç å¹¶ç›¸åŠ 
        #self.pe [:,:seq_len]çš„shape[1,seq_len,d_model]
        #å¹¿æ’­æœºåˆ¶ä¼šè‡ªåŠ¨æ‰©å±•åˆ°batchçº¬åº¦
        output = x +self.pe[:,:seq_len]

        return output

    def scaled_dot_product_attition(
            quary:torch.Tensor,
            key:torch.Tensor,
            value:torch.Tensor,
            mask:Optional[torch.Tensor]=None,
            drop_out:Optional[nn.Dropout] = None
    ) -> Tuple[torch.Tensor,torch.Tensor]:
        """
        ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶ï¼š
        å…¬å¼ï¼šAttention(Q,K,V) =softmax(Q*k^T/sqrt(d_k))V
            K^T:Kçš„è½¬èŒ
            d_k:Kçš„ç»´åº¦
        ä¸ºä¹ˆè¦è¿›è¡Œæ³¨æ„åŠ›ç¼©æ”¾ï¼Ÿ
            -å½“d_kå¾ˆå¤§æ—¶ï¼Œç‚¹ç§¯ç»“æœä¼šå¾ˆå°
            -å¯¼è‡´softmaxåçš„æ¢¯åº¦å¾ˆå°ï¼Œè®­ç»ƒå›°éš¾
            -é™¤ä»¥sqrt(d_k)å¯ä»¥è§£å†³è¿™ç§é—®é¢˜
        å‚æ•°ï¼š
            quary:[batch_size,n_heads,seq_len,d_k] -æŸ¥è¯¢çŸ©é˜µ
            key:[batch_size,n_heads,seq_len,d_k]-é”®çŸ©é˜µ
            value:[batch_size,n_heads,seq_len,d_k]- å€¼çŸ©é˜µ
            mask:[batch_size,1,1,seq_len] or [batch_size,1,seq_len,seq_len] -æ©ç 
            drop_out:Drop_outå±‚ï¼ˆå¯é€‰ï¼‰
        è¿”å›ï¼š

        æ•°æ®æµç¤ºä¾‹ï¼š
            
        """



def test_positional_encoding():
    """æµ‹è¯•ä½ç½®ç¼–ç æ¨¡å—"""
    print("\n" + "="*50)
    print("ğŸ§ª æµ‹è¯•ä½ç½®ç¼–ç ")
    print("="*50)
    
    batch_size = 2
    seq_len = 10
    d_model = 8
    
    # åˆ›å»ºæ¨¡æ‹Ÿè¾“å…¥
    x = torch.randn(batch_size, seq_len, d_model)
    print(f"è¾“å…¥shape: {x.shape}")
    
    # åˆ›å»ºä½ç½®ç¼–ç å±‚
    pos_encoder = PositionalEncoding(d_model, max_len=100)
    
    # å‰å‘ä¼ æ’­
    output = pos_encoder(x)
    print(f"è¾“å‡ºshape: {output.shape}")
    print(f"âœ… ä½ç½®ç¼–ç æµ‹è¯•é€šè¿‡ï¼\n")
    
    return output





def check_cuda_torch_info():
    torch_version = torch.__version__
    print(f"(â—'â—¡'â—)ğŸ”torchçš„ç‰ˆæœ¬:{torch_version}")
    cuda_venv_version = torch.version.cuda
    print(f"ğŸ”è™šæ‹Ÿç¯å¢ƒçš„cudaç‰ˆæœ¬:{cuda_venv_version}")
    cuda_aviable = torch.cuda.is_available()
    print(f"cuda å¯ç”¨ğŸ‘Œ" if cuda_aviable else "cuda ä¸å¯ç”¨ğŸ˜’")
    if cuda_aviable:
        gpu_count =torch.cuda.device_count()
        print(f"å¯ç”¨æ ¸å¿ƒæ•°ï¼š{gpu_count}")
        current_device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_device)
        print(f"å½“å‰Gpuç´¢å¼•{current_device},åç§°{gpu_name}")
        cuda_runtime_version = torch.backends.cudnn.version()
        print(f"cudnnçš„ç‰ˆæœ¬æ˜¯{cuda_runtime_version}")



if __name__ == '__main__':
    _ = test_positional_encoding()
    # å…ˆæ¿€æ´»ç¯å¢ƒï¼Œå†è¿è¡Œpython

