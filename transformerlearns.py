import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
import math
from typing import Optional,Tuple

torch.manual_seed(42)
np.random.seed(42)#è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯å¤ç°

class PoizitionalEnconding(nn.Module):
    """ä½ç½®ç¼–ç æ¨¡å—:ä¸ºåºåˆ—ä¸­æ¯ä¸ªä½ç½®æ·»åŠ ä½ç½®ä¿¡æ¯
    ä¸ºä»€ä¹ˆéœ€è¦ä½ç½®ç¼–ç ï¼Ÿ
    -transfomerä¸åƒRNNï¼Œæ²¡æœ‰å¾ªç¯ç»“æ„ï¼Œæœ¬èº«æ— æ³•æ„ŸçŸ¥è¯çš„é¡ºåº
    -éœ€è¦æ˜¾ç¤ºçš„å‘Šè¯‰æ¨¡å‹æ¯ä¸ªè¯åœ¨åºåˆ—ä¸­çš„ä½ç½®

    

    """

def check_cuda_torch_info():
    torch_version = torch.__version__
    print(f"(â—'â—¡'â—)ğŸ”torchçš„ç‰ˆæœ¬:{torch_version}")
    cuda_venv_version = torch.version.cuda
    print(f"ğŸ”è™šæ‹Ÿç¯å¢ƒçš„cudaç‰ˆæœ¬:{cuda_venv_version}")
    cuda_aviable = torch.cuda.is_available()
    print(f"cuda å¯ç”¨ğŸ‘Œ" if cuda_aviable else "cuda ä¸å¯ç”¨ğŸ˜’")
    if cuda_aviable:
        gpu_count =torch.cuda.device_count()
        print(f"å¯ç”¨æ ¸å¿ƒæ•°ï¼š{gpu_count}")
        current_device = torch.cuda.current_device()
        gpu_name = torch.cuda.get_device_name(current_device)
        print(f"å½“å‰Gpuç´¢å¼•{current_device},åç§°{gpu_name}")
        cuda_runtime_version = torch.backends.cudnn.version()
        print(f"cudnnçš„ç‰ˆæœ¬æ˜¯{cuda_runtime_version}")



if __name__ == '__main__':
    check_cuda_torch_info()
    # å…ˆæ¿€æ´»ç¯å¢ƒï¼Œå†è¿è¡Œpython

